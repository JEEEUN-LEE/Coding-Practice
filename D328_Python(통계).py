# -------------------------------
# 1. 머신러닝의 분류
# -------------------------------
# - 지도학습(Supervised Learning): 입력(X)과 정답(Y)이 있는 데이터 학습
#     예: 선형회귀, 로지스틱회귀, 의사결정나무 등
# - 비지도학습(Unsupervised Learning): 정답(Y)이 없는 데이터 구조 탐색
#     예: 군집분석(K-means), 주성분분석(PCA)
# - 강화학습(Reinforcement Learning): 보상을 기반으로 한 의사결정 학습
# - 룰베이스(Rule-based) 머신러닝: 사람이 정한 규칙 기반 예측 시스템
#     예: 군집분석(K-means), 주성분분석(PCA)
# - 강화학습(Reinforcement Learning): 보상을 기반으로 한 의사결정 학습
# - 룰베이스(Rule-based) 머신러닝: 사람이 정한 규칙 기반 예측 시스템

# -------------------------------
# 2. 통계학과 머신러닝의 차이
# -------------------------------
# - 통계모델(statistical model): 데이터를 생성하는 과정(모집단 분포)을 이해하는 목적
# - 머신러닝(machine learning): 새로운(미지의) 데이터의 예측 정확도를 높이는 것이 목적
# ※ 예: 로지스틱 회귀는 통계학에서도, 머신러닝에서도 사용됨

# -------------------------------
# 3. 정규화 (Regularization)
# -------------------------------
# - 회귀계수의 크기를 억제하기 위한 제약을 손실함수에 추가하는 기법
# - 과적합(overfitting) 방지 목적
# - 주요 기법: 리지 회귀(L2), 라소 회귀(L1)

# -------------------------------
# 4. 리지 회귀 (Ridge Regression)
# -------------------------------
# - L2 정규화 적용: 패널티 항으로 "계수 제곱합" 사용
# - 벌칙항: λ * Σ(β_i²)
# - 잔차제곱합(RSS)을 최소화하면서 계수가 너무 커지는 것을 방지
# - 결과적으로, 계수의 절댓값이 작아지는 "축소 추정(Shrinkage Estimation)" 발생

# -------------------------------
# 5. 라소 회귀 (Lasso Regression)
# -------------------------------
# - L1 정규화 적용: 패널티 항으로 "계수 절댓값의 합" 사용
# - 벌칙항: λ * Σ|β_i|
# - 계수 일부를 0으로 만들어 변수 선택(feature selection)에 효과적

# -------------------------------
# 6. 리지 vs 라소 비교
# -------------------------------
# 리지 회귀: 모든 변수를 남기되, 계수를 작게 만듦 (연속적 축소)
# 라소 회귀: 일부 변수를 완전히 제거 (희소성 유도)
# Elastic Net: L1 + L2 혼합 형태

# -------------------------------
# 7. 변수 표준화(Standardization)
# -------------------------------
# - 리지나 라소 회귀 수행 전, 독립변수의 단위를 맞추기 위해 필수
# - 평균을 0, 표준편차를 1로 맞춤
# - 이유: 변수 단위가 다르면 정규화 패널티의 영향을 다르게 받기 때문

# -------------------------------
# 8. 변수 선택 (Model Selection)
# -------------------------------
# - AIC, BIC 등의 기준으로 불필요한 변수를 제거
# - 단순하고 해석 가능한 모델을 목표로 함


# -------------------------------
# 9. 실습 코드: 데이터 표준화 및 회귀 모델링
# -------------------------------
import numpy as np
import scipy as sp
import pandas as pd
import statsmodels.api as sm

# 가상의 데이터 예시
np.random.seed(0)
X = np.random.randn(100, 3)
y = 3 * X[:, 0] + 2 * X[:, 1] + np.random.randn(100)

# -------------------------------
# (1) 데이터 표준화
# -------------------------------
X = pd.DataFrame(X, columns=['x1', 'x2', 'x3'])
print("표준화 전 평균:\n", X.mean().head(3))
print("표준화 전 표준편차:\n", X.std().head(3))

# 표준화 수행
X = (X - sp.mean(X, axis=0)) / sp.std(X, ddof=1, axis=0)

print("\n표준화 후 평균:\n", X.mean().head(3))
print("표준화 후 표준편차:\n", X.std().head(3))

# -------------------------------
# (2) 선형회귀 (OLS)
# -------------------------------
lm_statsmodels = sm.OLS(endog=y, exog=sm.add_constant(X)).fit()
print("\n회귀계수:\n", lm_statsmodels.params.head(3))
print("\n요약 결과:\n", lm_statsmodels.summary())
